# 模型管理

您可以通过导航到 `模型` 页面来管理 aiMindServe 中的大型语言模型。aiMindServe 中的模型包含一个或多个模型实例的副本。在部署时，aiMindServe 会自动从模型元数据计算模型实例的资源需求，并相应地将其调度到可用的工作节点上。

## 部署模型

目前支持来自 [Hugging Face](https://huggingface.co)、[ModelScope](https://modelscope.cn)、[Ollama](https://ollama.com/library) 和本地路径的模型。

### 部署 Hugging Face 模型

1. 点击 `部署模型` 按钮，然后在下拉菜单中选择 `Hugging Face`。

2. 使用左上角的搜索栏从 Hugging Face 按名称搜索模型。例如，`microsoft/Phi-3-mini-4k-instruct-gguf`。如果您只想搜索 GGUF 模型，请勾选 "GGUF" 复选框。

3. 从 `可用文件` 中选择具有所需量化格式的文件。

4. 根据需要调整 `名称` 和 `副本数`。

5. 如果需要，展开 `高级` 部分进行高级配置。有关更多详细信息，请参阅[高级模型配置](#高级模型配置)部分。

6. 点击 `保存` 按钮。

### 部署 ModelScope 模型

1. 点击 `部署模型` 按钮，然后在下拉菜单中选择 `ModelScope`。

2. 使用左上角的搜索栏从 ModelScope 按名称搜索模型。例如，`Qwen/Qwen2-0.5B-Instruct`。如果您只想搜索 GGUF 模型，请勾选 "GGUF" 复选框。

3. 从 `可用文件` 中选择具有所需量化格式的文件。

4. 根据需要调整 `名称` 和 `副本数`。

5. 如果需要，展开 `高级` 部分进行高级配置。有关更多详细信息，请参阅[高级模型配置](#高级模型配置)部分。

6. 点击 `保存` 按钮。


### 部署本地路径模型

您可以从本地路径部署模型。模型路径可以是位于工作节点上的目录（例如，下载的 Hugging Face 模型目录）或文件（例如，GGUF 模型文件）。这在运行于隔离环境时特别有用。

!!! 注意

    1. aiMindServe 不会检查模型路径的有效性以进行调度，如果模型路径无法访问，可能会导致部署失败。建议确保模型路径在所有工作节点上都可以访问（例如，使用 NFS、rsync 等）。您也可以使用工作节点选择器配置将模型部署到特定的工作节点。
    2. 除非服务器可以访问相同的模型路径，否则 aiMindServe 无法评估模型的资源需求。因此，您可能会观察到已部署模型的 VRAM/RAM 分配为空。为了缓解这个问题，建议在服务器上的相同路径提供模型文件。或者，您可以自定义后端参数，例如 `tensor-split`，以配置模型如何在 GPU 之间分配。

要部署本地路径模型：

1. 点击 `部署模型` 按钮，然后在下拉菜单中选择 `本地路径`。

2. 填写模型的 `名称`。

3. 填写 `模型路径`。

4. 根据需要调整 `副本数`。

5. 如果需要，展开 `高级` 部分进行高级配置。有关更多详细信息，请参阅[高级模型配置](#高级模型配置)部分。

6. 点击 `保存` 按钮。

## 编辑模型

1. 在模型列表页面找到您想要编辑的模型。
2. 点击 `操作` 列中的 `编辑` 按钮。
3. 根据需要更新属性。例如，更改 `副本数` 以扩展或缩减。
4. 点击 `保存` 按钮。

!!! 注意

    编辑模型后，配置不会应用于现有的模型实例。您需要删除现有的模型实例。aiMindServe 将根据更新后的模型配置重新创建新实例。

## 停止模型

停止模型将删除所有模型实例并释放资源。这相当于将模型缩减到零个副本。

1. 在模型列表页面找到您想要停止的模型。
2. 点击 `操作` 列中的省略号按钮，然后选择 `停止`。
3. 确认操作。

## 启动模型

启动模型相当于将模型扩展到一个副本。

1. 在模型列表页面找到您想要启动的模型。
2. 点击 `操作` 列中的省略号按钮，然后选择 `启动`。

## 删除模型

1. 在模型列表页面找到您想要删除的模型。
2. 点击 `操作` 列中的省略号按钮，然后选择 `删除`。
3. 确认删除。

## 查看模型实例

1. 在模型列表页面找到您想要检查的模型。
2. 点击 `>` 符号查看模型的实例列表。

## 删除模型实例

1. 在模型列表页面找到您想要检查的模型。
2. 点击 `>` 符号查看模型的实例列表。
3. 找到您想要删除的模型实例。
4. 点击模型实例在 `操作` 列中的省略号按钮，然后选择 `删除`。
5. 确认删除。

!!! 注意

    删除模型实例后，如果需要，aiMindServe 将重新创建一个新实例以满足模型的预期副本数。

## 查看模型实例日志

1. 在模型列表页面找到您想要检查的模型。
2. 点击 `>` 符号查看模型的实例列表。
3. 找到您想要检查的模型实例。
4. 点击模型实例在 `操作` 列中的 `查看日志` 按钮。


## 高级模型配置

aiMindServe 支持为模型部署定制配置。

### 模型类别

模型类别帮助您组织和过滤模型。默认情况下，aiMindServe 会根据模型的元数据自动检测模型类别。您也可以通过从下拉列表中选择来自定义类别。

### 调度类型

#### 自动

aiMindServe根据当前资源可用性自动将模型实例调度到适当的 GPU/工作节点。

- 放置策略

  - 分散：使整个集群的资源相对均匀地分布在所有工作节点之间。这可能会在单个工作节点上产生更多的资源碎片。

  - 装箱：优先考虑集群资源的整体利用率，减少工作节点/GPU 上的资源碎片。

- 工作节点选择器

  配置后，调度器将把模型实例部署到包含指定标签的工作节点。

  1. 导航到 `资源` 页面并编辑所需的工作节点。通过在标签部分添加自定义标签来为工作节点分配标签。

  2. 转到 `模型` 页面并点击 `部署模型` 按钮。展开 `高级` 部分并在 `工作节点选择器` 配置中输入之前分配的工作节点标签。在部署期间，模型实例将根据这些标签分配到相应的工作节点。

#### 手动

此调度类型允许用户指定要在哪个 GPU 上部署模型实例。

- GPU 选择器

  从列表中选择一个或多个 GPU。如果资源允许，模型实例将尝试部署到选定的 GPU。

### 后端

推理后端。目前，aiMindServe 支持三个后端：llama-box、vLLM 和 vox-box。aiMindServe 会根据模型的配置自动选择后端。

有关更多详细信息，请参阅[推理后端](./inference-backends.md)部分。

### 后端版本

指定后端版本，例如 `v1.0.0`。版本格式和可用性取决于所选的后端。此选项对于确保兼容性或利用特定后端版本中引入的功能很有用。有关更多信息，请参阅[固定后端版本](./pinned-backend-versions.md)部分。

### 后端参数

输入您想要在运行模型时自定义的后端参数。参数格式应为 `--parameter=value`、`--bool-parameter` 或作为 `--parameter` 和 `value` 的单独字段。
例如，对于 llama-box 使用 `--ctx-size=8192`。

有关支持的参数的完整列表，请参阅[推理后端](./inference-backends.md)部分。

### 环境变量

运行模型时使用的环境变量。这些变量在启动时传递给后端进程。

### 允许 CPU 卸载

!!! 注意

    仅适用于 llama-box 后端。

启用 CPU 卸载后，aiMindServe 会优先将尽可能多的层加载到 GPU 上以优化性能。如果 GPU 资源有限，一些层将被卸载到 CPU，只有在没有 GPU 可用时才使用完整的 CPU 推理。

### 允许跨工作节点分布式推理

!!! 注意

    仅适用于 llama-box 和 vLLM 后端。

启用跨多个工作节点的分布式推理。主模型实例将与一个或多个其他工作节点上的后端实例通信，将计算任务卸载给它们。

### 错误时自动重启

如果模型实例遇到错误，启用自动重启。此功能确保模型实例的高可用性和可靠性。如果发生错误，aiMindServe 将使用指数退避策略自动尝试重启模型实例。重启尝试之间的延迟呈指数增长，最长间隔为 5 分钟。这种方法可以防止系统在持续错误的情况下因频繁重启而超载。 